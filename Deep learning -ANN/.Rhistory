setwd("~/Documents/Machine Learning/Git/ML_AI_Practice_Projects/Natural Language Processing")
dataset = read.delim('Restaurant_Reviews.tsv', header = TRUE, quote = "", stringsAsFactors = FALSE)
library(tm)
corpus = VCorpus(VectorSource(dataset$Review))
View(corpus)
corpus = tm_map(corpus, content_transformer(tolower)) #everything to lower case
as.character(corpus[1])
as.character(corpus[[1]])
corpus = tm_map(corpus, removeNumbers) #remove numbers
corpus = tm_map(corpus, removePunctuation) #remove punctuation
install.packages('SnowballC')
library(SnowballC)
corpus = tm_map(corpus, removeWords, stopwords()) #remove irrelevant words
as.character(corpus[[1]])
corpus = tm_map(corpus, stemDocument) #stemming
as.character(corpus[[1]])
as.character(corpus[[2]])
corpus = tm_map(corpus, stripWhitespace) #removing extra spaces
as.character(corpus[[1]])
dtm = DocumentTermMatrix(corpus)
View(dtm)
dtm[["v"]]
dtm[["ncol"]]
dtm
dtm = removeSparseTerms(dtm, 0.99)
dtm
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
View(dataset)
dataset_original = read.delim('Restaurant_Reviews.tsv', header = TRUE, quote = "", stringsAsFactors = FALSE)
dataset$Liked = dataset_original$Liked
dataset_original$Liked
View(dataset_original)
#Encoding the target feature as factor
dataset$Liked = factor(dataset$Liked,
levels = c(0,1))
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting Random Forest classifier to the Training Set
library(randomForest)
classifier = randomForest(x = training_set[-692],
y = training_set$Liked,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-692])
# Making the confusion Matrix
cm = table(test_set[,692], y_pred)
cm
dim(dataset)
dim(dataset[1])
dim(dataset)[1]
dim(dataset)[2]
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting Random Forest classifier to the Training Set
library(randomForest)
n = dim(dataset)[2]
classifier = randomForest(x = training_set[-n],
y = training_set$Liked,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-n])
# Making the confusion Matrix
cm = table(test_set[,n], y_pred)
# Natural Language Processing
# Importing the dataset
dataset_original = read.delim('Restaurant_Reviews.tsv', header = TRUE, quote = "", stringsAsFactors = FALSE)
# Cleaning the text
#install.packages('tm')
# install.packages('SnowballC')
library(tm)
library(SnowballC)
corpus = VCorpus(VectorSource(dataset_original$Review))
# to view
# as.character(corpus[[1]])
corpus = tm_map(corpus, content_transformer(tolower)) #everything to lower case
corpus = tm_map(corpus, removeNumbers) #remove numbers
corpus = tm_map(corpus, removePunctuation) #remove punctuation
corpus = tm_map(corpus, removeWords, stopwords()) #remove irrelevant words
corpus = tm_map(corpus, stemDocument) #stemming
corpus = tm_map(corpus, stripWhitespace) #removing extra spaces
# Creating the bag of words model
dtm = DocumentTermMatrix(corpus)
dtm = removeSparseTerms(dtm, 0.999)
dataset = as.data.frame(as.matrix(dtm))
dataset$Liked = dataset_original$Liked
# Random forest classification model
#Encoding the target feature as factor
dataset$Liked = factor(dataset$Liked,
levels = c(0,1))
library(caTools)
set.seed(123)
split = sample.split(dataset$Liked, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
# Fitting Random Forest classifier to the Training Set
library(randomForest)
n = dim(dataset)[2]
classifier = randomForest(x = training_set[-n],
y = training_set$Liked,
ntree = 10)
# Predicting the Test set results
y_pred = predict(classifier, newdata = test_set[-n])
# Making the confusion Matrix
cm = table(test_set[,n], y_pred)
cm
setwd("~/Documents/Machine Learning/Git/ML_AI_Practice_Projects/Deep learning -ANN")
dataset = read.csv('Churn_Modelling.csv')
View(dataset)
dataset = dataset[4:]
dataset = dataset[4:14]
View(dataset)
# Encoding the categorical variables as factor
dataset$Geography = as.numeric(factor(dataset$Geography,
levels = c('France', 'Spain', 'Germany'),
labels = c(1,2,3)))
dataset$Gender = as.numeric(factor(dataset$Gender,
levels = c('Female', 'Male'),
labels = c(1,2)))
View(dataset)
library(caTools)
set.seed(123)
split = sample.split(dataset$Exited, SplitRatio = 0.8)
training_set = subset(dataset, split == TRUE)
test_set = subset(dataset, split == FALSE)
View(training_set)
View(test_set)
#Feature Scaling
training_set[-11] = scale(training_set[-11])
test_set[-11] = scale(test_set[-11])
View(training_set)
install.packages("h2o")
library(h2o)
h2o.init(nthreads = -1) #nthreads = -1 takes all available cores
h2o.init(nthreads = -1) #nthreads = -1 takes all available cores
